{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklasgrimm98/Digital-Organization/blob/main/homework_4_niklas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "In the following assignment, your task is to complete the MNIST Basics chapter. It is best to repeat everything from last week and try to answer the following questions. Afterwards you have to summarize the learned facts with two programming tasks.\n",
        "\n",
        "What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?\n",
        "\n",
        "The torch.cat transforms the object from a list of matrices to a list of vecotrs. It changes th shape of the tensor from rank-3 to a rank-2 tensor. \n",
        "\n",
        "\n",
        "Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Why is it not possible to use the accuracy as loss function?\n",
        "\n",
        "Since changing the weights only a little bit will often not change the accuracy of the predictions, it cannot be used as the loss function. We need a more fine grained scale, so a slightly different weight gives us a slightly better prediction. \n",
        "\n",
        "What is the defined `mnist_loss` function doing? \n",
        "\n",
        "The funtion mnist_loss return the mean of how distant the predictions are from the actual values. So if the acutal value is a 3 with how much confidence does the model predict it to be a 3. \n",
        "\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```\n",
        "\n",
        "Why do we need additionaly the sigmoid() function? What is it technically in our TLU?\n",
        "\n",
        "The sigmoid function is our activation function. It transform all input values and smoothes them into an output value between 0 and 1. \n",
        "\n",
        "Again, what are mini batches, why are we using them and why should they be shuffeld? \n",
        "\n",
        "Since calculating the loss over the whole dataset is to time-consuming and calculating it for only one item does not result in a lot if information, we calculate it using a mini-batch. We shuffle the dataset on every epoch to vary things during the training process in order to get better generalization. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "fastbook.setup_book()\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klYrjjv4pNCH",
        "outputId": "d4ecdb4c-7611-4023-e23c-490fb2b9465b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "TxwyNuzj3DYu"
      },
      "outputs": [],
      "source": [
        "#YOUR TASK: Manually train a single layer perceptron without using an optimizer.\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')\n",
        "path = untar_data(URLs.MNIST)\n",
        "Path.BASE_PATH = path\n",
        "\n",
        "ones = ((path/'training'/'1').ls() + (path/'testing'/'1').ls()).sorted()\n",
        "sevens = ((path/'training'/'7').ls() + (path/'testing'/'7').ls()).sorted()\n",
        "\n",
        "valid_1_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'testing'/'1').ls()])\n",
        "valid_1_tens = valid_1_tens.float()/255\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'testing'/'7').ls()])\n",
        "valid_7_tens = valid_7_tens.float()/255\n",
        "\n",
        "one_tensors = [tensor(Image.open(o)) for o in ones]\n",
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "\n",
        "stacked_ones = torch.stack(one_tensors).float()/255\n",
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "\n",
        "ones_train, ones_test = train_test_split(stacked_ones, test_size=0.2, random_state = 10)\n",
        "sevens_train, sevens_test = train_test_split(stacked_sevens, test_size=0.2, random_state = 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
        "\n",
        "def linear1(xb): return xb@weights + bias\n",
        "\n",
        "\n",
        "def sigmoid(x): return 1/(1+torch.exp(-x))\n",
        "\n",
        "def mnist_loss(predictions, targets):\n",
        "    predictions = predictions.sigmoid()\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "def calc_grad(xb, yb, model):\n",
        "    preds = model(xb)\n",
        "    loss = mnist_loss(preds, yb)\n",
        "    loss.backward()\n",
        "\n",
        "def train_epoch(model, lr, params):\n",
        "    for xb,yb in dl:\n",
        "        calc_grad(xb, yb, model)\n",
        "        for p in params:\n",
        "            p.data -= p.grad*lr\n",
        "            p.grad.zero_()\n",
        "\n",
        "def batch_accuracy(xb, yb):\n",
        "    preds = xb.sigmoid()\n",
        "    correct = (preds>0.5) == yb\n",
        "    return correct.float().mean()\n",
        "\n",
        "def validate_epoch(model):\n",
        "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
        "    return round(torch.stack(accs).mean().item(), 4)\n",
        "\n",
        "class BasicOptim:\n",
        "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
        "\n",
        "    def step(self, *args, **kwargs):\n",
        "        for p in self.params: p.data -= p.grad.data * self.lr\n",
        "\n",
        "    def zero_grad(self, *args, **kwargs):\n",
        "        for p in self.params: p.grad = None\n",
        "\n",
        "def train_epoch2(model):\n",
        "    for xb,yb in dl:\n",
        "        calc_grad(xb, yb, model)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "def train_model(model, epochs):\n",
        "    for i in range(epochs):\n",
        "        train_epoch2(model)\n",
        "        print(validate_epoch(model), end=' ')\n",
        "\n",
        "def simple_net(xb): \n",
        "    res = xb@w1 + b1\n",
        "    res = res.max(tensor(0.0))\n",
        "    res = res@w2 + b2\n",
        "    return res"
      ],
      "metadata": {
        "id": "se8cX38bCE5C"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.cat([stacked_ones, stacked_sevens]).view(-1, 28*28)\n",
        "train_y = tensor([1]*len(ones) + [0]*len(sevens)).unsqueeze(1)\n",
        "dset = list(zip(train_x,train_y))\n",
        "\n",
        "valid_x = torch.cat([valid_1_tens, valid_7_tens]).view(-1, 28*28)\n",
        "valid_y = tensor([1]*len(valid_1_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
        "valid_dset = list(zip(valid_x,valid_y))\n",
        "\n",
        "weights = init_params((28*28,1))\n",
        "bias = init_params(1)\n",
        "\n",
        "dl = DataLoader(dset, batch_size=256)\n",
        "xb,yb = first(dl)\n",
        "xb.shape,yb.shape\n",
        "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
        "\n",
        "\n",
        "lr = 1.\n",
        "params = weights,bias\n",
        "train_epoch(linear1, lr, params)\n",
        "validate_epoch(linear1)\n",
        "\n",
        "for i in range(20):\n",
        "    train_epoch(linear1, lr, params)\n",
        "    print(validate_epoch(linear1), end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciD4R1Cm7Uao",
        "outputId": "6b778679-0466-479e-e746-e819c6c444ae"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9451 0.9668 0.9768 0.9803 0.9807 0.983 0.9843 0.9847 0.9851 0.986 0.9869 0.9877 0.9877 0.9885 0.988 0.9884 0.9884 0.9889 0.9889 0.9893 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR TASK: Train a simple two-layer neural network (two perceptrons + hidden activation function) with built-in functions and an optimizer.\n",
        "\n",
        "linear_model = nn.Linear(28*28,1)\n",
        "w,b = linear_model.parameters()\n",
        "w.shape,b.shape\n",
        "\n",
        "opt = BasicOptim(linear_model.parameters(), lr)\n",
        "\n",
        "\n",
        "train_model(linear_model, 20)\n",
        "\n",
        "linear_model = nn.Linear(28*28,1)\n",
        "opt = SGD(linear_model.parameters(), lr)\n",
        "train_model(linear_model, 20)\n",
        "\n",
        "dls = DataLoaders(dl, valid_dl)"
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7940fff-e9dd-45d9-ce4c-d2ce408d686c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6632 0.898 0.9618 0.9759 0.9855 0.9877 0.9903 0.9907 0.9916 0.9924 0.992 0.992 0.992 0.992 0.992 0.9924 0.9929 0.9924 0.9924 0.9924 0.6528 0.8989 0.9605 0.9765 0.9855 0.9885 0.9903 0.9911 0.992 0.9924 0.9924 0.992 0.992 0.992 0.992 0.9924 0.9929 0.9929 0.9929 0.9929 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = init_params((28*28,30))\n",
        "b1 = init_params(30)\n",
        "w2 = init_params((30,1))\n",
        "b2 = init_params(1)\n",
        "\n",
        "\n",
        "simple_net = nn.Sequential(\n",
        "    nn.Linear(28*28,30),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30,1)\n",
        ")\n",
        "\n",
        "learn = Learner(dls, simple_net, opt_func=SGD,\n",
        "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
        "\n",
        "learn.fit(40, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kfYZrdtaJHHH",
        "outputId": "58d96977-48c0-418f-e9f1-1f93ef2366ac"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>batch_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.224414</td>\n",
              "      <td>0.349287</td>\n",
              "      <td>0.556172</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.091451</td>\n",
              "      <td>0.130336</td>\n",
              "      <td>0.947295</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.047754</td>\n",
              "      <td>0.061737</td>\n",
              "      <td>0.980120</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.031099</td>\n",
              "      <td>0.040592</td>\n",
              "      <td>0.986593</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.023812</td>\n",
              "      <td>0.031038</td>\n",
              "      <td>0.988904</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.020096</td>\n",
              "      <td>0.025684</td>\n",
              "      <td>0.990291</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.017883</td>\n",
              "      <td>0.022270</td>\n",
              "      <td>0.990291</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.016376</td>\n",
              "      <td>0.019910</td>\n",
              "      <td>0.990291</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.015246</td>\n",
              "      <td>0.018181</td>\n",
              "      <td>0.990754</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.014348</td>\n",
              "      <td>0.016854</td>\n",
              "      <td>0.990291</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.013609</td>\n",
              "      <td>0.015801</td>\n",
              "      <td>0.990754</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.012987</td>\n",
              "      <td>0.014942</td>\n",
              "      <td>0.990754</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.012455</td>\n",
              "      <td>0.014229</td>\n",
              "      <td>0.990754</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.011994</td>\n",
              "      <td>0.013628</td>\n",
              "      <td>0.991216</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.011589</td>\n",
              "      <td>0.013115</td>\n",
              "      <td>0.991216</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.011230</td>\n",
              "      <td>0.012671</td>\n",
              "      <td>0.991216</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.010909</td>\n",
              "      <td>0.012283</td>\n",
              "      <td>0.991216</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.010621</td>\n",
              "      <td>0.011939</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.010358</td>\n",
              "      <td>0.011631</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.010118</td>\n",
              "      <td>0.011351</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.009898</td>\n",
              "      <td>0.011094</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.009694</td>\n",
              "      <td>0.010856</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.009504</td>\n",
              "      <td>0.010632</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.009326</td>\n",
              "      <td>0.010419</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.009159</td>\n",
              "      <td>0.010216</td>\n",
              "      <td>0.991678</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.009002</td>\n",
              "      <td>0.010021</td>\n",
              "      <td>0.992141</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>0.009832</td>\n",
              "      <td>0.992603</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.008712</td>\n",
              "      <td>0.009650</td>\n",
              "      <td>0.993065</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.008578</td>\n",
              "      <td>0.009475</td>\n",
              "      <td>0.993528</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.008450</td>\n",
              "      <td>0.009307</td>\n",
              "      <td>0.993528</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.008327</td>\n",
              "      <td>0.009146</td>\n",
              "      <td>0.993990</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.008210</td>\n",
              "      <td>0.008993</td>\n",
              "      <td>0.993990</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.008099</td>\n",
              "      <td>0.008848</td>\n",
              "      <td>0.993990</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.007992</td>\n",
              "      <td>0.008713</td>\n",
              "      <td>0.994452</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.007890</td>\n",
              "      <td>0.008586</td>\n",
              "      <td>0.994452</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.007793</td>\n",
              "      <td>0.008467</td>\n",
              "      <td>0.994452</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.007700</td>\n",
              "      <td>0.008356</td>\n",
              "      <td>0.994452</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.007612</td>\n",
              "      <td>0.008253</td>\n",
              "      <td>0.994452</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.007529</td>\n",
              "      <td>0.008157</td>\n",
              "      <td>0.994452</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.007450</td>\n",
              "      <td>0.008067</td>\n",
              "      <td>0.994452</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(L(learn.recorder.values).itemgot(2));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "kg2WThJaN2Sw",
        "outputId": "12c4c874-5027-4ae1-a4ba-32e16429c86a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}